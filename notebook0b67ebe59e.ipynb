{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13123991,"sourceType":"datasetVersion","datasetId":8313645}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:53:04.927354Z","iopub.execute_input":"2025-09-21T21:53:04.927766Z","iopub.status.idle":"2025-09-21T21:53:08.221680Z","shell.execute_reply.started":"2025-09-21T21:53:04.927738Z","shell.execute_reply":"2025-09-21T21:53:08.220855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Define the base dataset path (read-only input)\ndataset_base = r\"/kaggle/input/dataset/dataset/dataset/dataset\"\n\n# Define the output directories (writable in Kaggle working directory)\noutput_base = r\"/kaggle/working/shayari_consolidated\"\nurdu_all_dir = os.path.join(output_base, \"Urdu_All\")\nroman_all_dir = os.path.join(output_base, \"Roman_All\")\n\n# Create the main output directories if they don't exist\nos.makedirs(urdu_all_dir, exist_ok=True)\nos.makedirs(roman_all_dir, exist_ok=True)\n\n# Get the list of poet folders\npoet_dirs = sorted(os.listdir(dataset_base))\n\n# Iterate through each poet's folder\nfor poet in poet_dirs:\n    poet_path = os.path.join(dataset_base, poet)\n    if os.path.isdir(poet_path):  # Ensure it's a directory\n        urdu_src_dir = os.path.join(poet_path, \"ur\")\n        roman_src_dir = os.path.join(poet_path, \"en\")\n\n        # Create poet subfolders in the consolidated directories\n        urdu_poet_dir = os.path.join(urdu_all_dir, poet)\n        roman_poet_dir = os.path.join(roman_all_dir, poet)\n        os.makedirs(urdu_poet_dir, exist_ok=True)\n        os.makedirs(roman_poet_dir, exist_ok=True)\n\n        # Copy Urdu files\n        if os.path.exists(urdu_src_dir):\n            urdu_files = os.listdir(urdu_src_dir)\n            for file_name in urdu_files:\n                src_file = os.path.join(urdu_src_dir, file_name)\n                dst_file = os.path.join(urdu_poet_dir, file_name)\n                if os.path.isfile(src_file):\n                    shutil.copy2(src_file, dst_file)\n                    print(f\"Copied Urdu file: {file_name} from {poet} to {urdu_poet_dir}\")\n\n        # Copy Roman/English files\n        if os.path.exists(roman_src_dir):\n            roman_files = os.listdir(roman_src_dir)\n            for file_name in roman_files:\n                src_file = os.path.join(roman_src_dir, file_name)\n                dst_file = os.path.join(roman_poet_dir, file_name)\n                if os.path.isfile(src_file):\n                    shutil.copy2(src_file, dst_file)\n                    print(f\"Copied Roman file: {file_name} from {poet} to {roman_poet_dir}\")\n\nprint(f\"\\nConsolidation complete!\")\nprint(f\"Urdu files in: {urdu_all_dir}\")\nprint(f\"Roman files in: {roman_all_dir}\")\nprint(\"Structure: Each poet has a subfolder inside Urdu_All and Roman_All containing their shayari files.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:53:39.017352Z","iopub.execute_input":"2025-09-21T21:53:39.018319Z","iopub.status.idle":"2025-09-21T21:53:51.033718Z","shell.execute_reply.started":"2025-09-21T21:53:39.018285Z","shell.execute_reply":"2025-09-21T21:53:51.032998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install jiwer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:36:56.101110Z","iopub.execute_input":"2025-09-21T16:36:56.101707Z","iopub.status.idle":"2025-09-21T16:36:59.284730Z","shell.execute_reply.started":"2025-09-21T16:36:56.101680Z","shell.execute_reply":"2025-09-21T16:36:59.283923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls -l /kaggle/working/shayari_consolidated/Urdu_All\n!ls -l /kaggle/working/shayari_consolidated/Roman_All\n!ls -l /kaggle/working/shayari_consolidated/Urdu_All/* | head -n 20\n!ls -l /kaggle/working/shayari_consolidated/Roman_All/* | head -n 20","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T21:54:15.275245Z","iopub.execute_input":"2025-09-21T21:54:15.275584Z","iopub.status.idle":"2025-09-21T21:54:16.019748Z","shell.execute_reply.started":"2025-09-21T21:54:15.275556Z","shell.execute_reply":"2025-09-21T21:54:16.018969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install jiwer\n!pip install jiwer\n\n# Libraries\nimport os\nimport re\nimport unicodedata\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nimport jiwer\nimport numpy as np\nimport random\nfrom tqdm import tqdm\n\n# Random seed\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Dataset paths\nURDU_BASE_DIR = '/kaggle/working/shayari_consolidated/Urdu_All'\nROMAN_BASE_DIR = '/kaggle/working/shayari_consolidated/Roman_All'\nprint(f\"Urdu base dir: {URDU_BASE_DIR}\")\nprint(f\"Roman base dir: {ROMAN_BASE_DIR}\")\n\n# Clean text function (preserves Roman diacritics)\ndef clean_text(text, is_urdu=True):\n    text = unicodedata.normalize('NFKC', text)\n    if is_urdu:\n        text = re.sub(r'[^\\u0600-\\u06FF\\u0750-\\u077F\\s\\!\\?.,]', '', text)\n    else:\n        # For Roman, keep diacritics and basic punctuation\n        text = re.sub(r'[^\\w\\s\\!\\?.,āīūḳḥñ]', '', text)\n    return text.strip()\n\n# Validate Urdu/Roman text\ndef is_urdu_text(text):\n    return any(ord(c) >= 0x0600 and ord(c) <= 0x06FF for c in text)\n\ndef is_roman_text(text):\n    return all(ord(c) < 0x0600 or c in ' !?.,āīūḳḥñ' for c in text)\n\n# Load data with validation\ndef load_shayari_pairs(urdu_base, roman_base, max_lines_per_file=1000):\n    pairs = []\n    poets = set()\n    \n    urdu_poets = [d for d in os.listdir(urdu_base) if os.path.isdir(os.path.join(urdu_base, d))]\n    roman_poets = [d for d in os.listdir(roman_base) if os.path.isdir(os.path.join(roman_base, d))]\n    common_poets = set(urdu_poets) & set(roman_poets)\n    print(f\"Found {len(common_poets)} common poets: {common_poets}\")\n    \n    if not common_poets:\n        print(\"No common poets found! Check directory structure.\")\n        return pairs\n    \n    for poet in tqdm(sorted(common_poets), desc=\"Loading poets\"):\n        urdu_poet_dir = os.path.join(urdu_base, poet)\n        roman_poet_dir = os.path.join(roman_base, poet)\n        \n        urdu_files = [f for f in os.listdir(urdu_poet_dir) if os.path.isfile(os.path.join(urdu_poet_dir, f))]\n        roman_files = [f for f in os.listdir(roman_poet_dir) if os.path.isfile(os.path.join(roman_poet_dir, f))]\n        print(f\"Poet: {poet}, Urdu files: {len(urdu_files)}, Roman files: {len(roman_files)}\")\n        print(f\"Sample Urdu files: {urdu_files[:2]}\")\n        print(f\"Sample Roman files: {roman_files[:2]}\")\n        \n        common_files = set(urdu_files) & set(roman_files)\n        print(f\"Poet: {poet}, Common files: {len(common_files)}\")\n        \n        for file_name in common_files:\n            ur_path = os.path.join(urdu_poet_dir, file_name)\n            rom_path = os.path.join(roman_poet_dir, file_name)\n            print(f\"Processing pair: {file_name}\")\n            \n            try:\n                with open(ur_path, 'r', encoding='utf-8-sig') as f_ur, \\\n                     open(rom_path, 'r', encoding='utf-8-sig') as f_rom:\n                    \n                    ur_lines = [clean_text(line, is_urdu=True) for line in f_ur.readlines() if line.strip()]\n                    rom_lines = [clean_text(line, is_urdu=False) for line in f_rom.readlines() if line.strip()]\n                    \n                    if ur_lines and rom_lines:\n                        f_ur.seek(0)\n                        f_rom.seek(0)\n                        print(f\"Sample raw Urdu lines: {f_ur.readlines()[:10]}\")\n                        print(f\"Sample cleaned Urdu: {ur_lines[:10]}\")\n                        print(f\"Sample raw Roman lines: {f_rom.readlines()[:10]}\")\n                        print(f\"Sample cleaned Roman: {rom_lines[:10]}\")\n                    \n                    ur_lines = [line for line in ur_lines if line and is_urdu_text(line)]\n                    rom_lines = [line for line in rom_lines if line and is_roman_text(line)]\n                    \n                    min_len = min(len(ur_lines), len(rom_lines))\n                    ur_lines = ur_lines[:min(min_len, max_lines_per_file)]\n                    rom_lines = rom_lines[:min(min_len, max_lines_per_file)]\n                    \n                    if min_len > 0:\n                        valid_pairs = [(ur, rom) for ur, rom in zip(ur_lines, rom_lines) if is_urdu_text(ur) and is_roman_text(rom)]\n                        pairs.extend(valid_pairs)\n                        poets.add(poet)\n                        print(f\"Loaded {len(valid_pairs)} pairs for poet: {poet} from {file_name}\")\n                    else:\n                        print(f\"No valid pairs for {file_name} (min_len={min_len})\")\n            except Exception as e:\n                print(f\"Error loading {file_name} for {poet}: {e}\")\n    \n    print(f\"Total pairs loaded: {len(pairs)} from {len(poets)} poets.\")\n    return pairs\n\n# Load pairs\npairs = load_shayari_pairs(URDU_BASE_DIR, ROMAN_BASE_DIR)\nif not pairs:\n    print(\"No pairs loaded! Check paths, file names, or content.\")\n    raise SystemExit\nelse:\n    print(f\"Sample pair: Urdu: '{pairs[0][0]}' | Roman: '{pairs[0][1]}'\")\n    \n    # Prepare sentences for word-level processing\n    urdu_sentences = [pair[0] for pair in pairs]\n    roman_sentences = [pair[1] for pair in pairs]\n    \n    # Data augmentation\n    AUG_MULTIPLIER = 3\n    urdu_sentences = urdu_sentences * AUG_MULTIPLIER\n    roman_sentences = roman_sentences * AUG_MULTIPLIER\n    swapped_pairs = [(rom, ur) for ur, rom in zip(urdu_sentences[:len(urdu_sentences)//AUG_MULTIPLIER], roman_sentences[:len(roman_sentences)//AUG_MULTIPLIER])]\n    urdu_sentences.extend([pair[0] for pair in swapped_pairs])\n    roman_sentences.extend([pair[1] for pair in swapped_pairs])\n    print(f\"After augmentation: {len(urdu_sentences)} sentences.\")\n    \n    import pickle\n    with open('/kaggle/working/sentences.pkl', 'wb') as f:\n        pickle.dump((urdu_sentences, roman_sentences), f)\n    print(\"Sentences saved to /kaggle/working/sentences.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:15:03.227364Z","iopub.execute_input":"2025-09-24T17:15:03.227932Z","iopub.status.idle":"2025-09-24T17:15:10.821688Z","shell.execute_reply.started":"2025-09-24T17:15:03.227906Z","shell.execute_reply":"2025-09-24T17:15:10.820875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set multiprocessing start method\nimport torch.multiprocessing as mp\ntry:\n    mp.set_start_method('spawn', force=True)\nexcept RuntimeError:\n    pass\n\n# Load sentences\nimport pickle\nwith open('/kaggle/working/sentences.pkl', 'rb') as f:\n    urdu_sentences, roman_sentences = pickle.load(f)\nprint(f\"Loaded {len(urdu_sentences)} sentence pairs from disk.\")\n\n# Vocabulary\nclass Vocab:\n    def __init__(self, texts, min_freq=1):\n        self.freq = {}\n        for t in texts:\n            for tok in t.split():\n                self.freq[tok] = self.freq.get(tok, 0) + 1\n        self.itos = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n        for tok, c in self.freq.items():\n            if c >= min_freq:\n                self.itos.append(tok)\n        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n    def encode(self, text):\n        return [self.stoi.get(tok, self.stoi[\"<unk>\"]) for tok in text.split()]\n    def decode(self, ids):\n        return [self.itos[i] for i in ids if i not in [0, 1, 2]]\n\n# Build vocabularies\nsrc_vocab = Vocab(urdu_sentences)\ntgt_vocab = Vocab(roman_sentences)\nprint(f\"Urdu vocab size: {len(src_vocab.itos)}, Roman vocab size: {len(tgt_vocab.itos)}\")\n\n# Dataset\nclass NMTDataset(Dataset):\n    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab, max_len=20):\n        self.src, self.tgt = src_texts, tgt_texts\n        self.src_vocab, self.tgt_vocab = src_vocab, tgt_vocab\n        self.max_len = max_len\n    def __len__(self): return len(self.src)\n    def __getitem__(self, idx):\n        src_ids = [1] + self.src_vocab.encode(self.src[idx])[:self.max_len-2] + [2]\n        tgt_ids = [1] + self.tgt_vocab.encode(self.tgt[idx])[:self.max_len-2] + [2]\n        src_ids = src_ids + [0]*(self.max_len-len(src_ids)) if len(src_ids) < self.max_len else src_ids[:self.max_len]\n        tgt_ids = tgt_ids + [0]*(self.max_len-len(tgt_ids)) if len(tgt_ids) < self.max_len else tgt_ids[:self.max_len]\n        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n\n# Data split\ntrain_idx, temp_idx = train_test_split(range(len(urdu_sentences)), test_size=0.2, random_state=42)\nval_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)\ntrain_urdu = [urdu_sentences[i] for i in train_idx]\ntrain_roman = [roman_sentences[i] for i in train_idx]\nval_urdu = [urdu_sentences[i] for i in val_idx]\nval_roman = [roman_sentences[i] for i in val_idx]\ntest_urdu = [urdu_sentences[i] for i in test_idx]\ntest_roman = [roman_sentences[i] for i in test_idx]\nprint(f\"Train: {len(train_urdu)}, Val: {len(val_urdu)}, Test: {len(test_urdu)}\")\n\ntrain_dataset = NMTDataset(train_urdu, train_roman, src_vocab, tgt_vocab)\nval_dataset = NMTDataset(val_urdu, val_roman, src_vocab, tgt_vocab)\ntest_dataset = NMTDataset(test_urdu, test_roman, src_vocab, tgt_vocab)\n\n# Collate function\ndef collate_fn(batch):\n    src_batch, tgt_batch = zip(*batch)\n    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=0, batch_first=True)\n    tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, padding_value=0, batch_first=True)\n    return src_batch, tgt_batch\n\n# Model classes\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, n_layers=2, dropout=0.3):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout if n_layers > 1 else 0,\n                            batch_first=True, bidirectional=True)\n    def forward(self, src):\n        emb = self.embedding(src)\n        outputs, (h, c) = self.lstm(emb)\n        return outputs, (h, c)\n\nclass LuongAttention(nn.Module):\n    def __init__(self, hidden_dim):\n        super().__init__()\n        self.attn = nn.Linear(hidden_dim*2, hidden_dim*2)\n    def forward(self, hidden, encoder_outputs):\n        scores = torch.bmm(encoder_outputs, self.attn(hidden).unsqueeze(2)).squeeze(2)\n        attn_weights = torch.softmax(scores, dim=1)\n        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n        return context.squeeze(1), attn_weights\n\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hidden_dim, n_layers=2, dropout=0.3):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n        self.lstm = nn.LSTM(emb_dim, hidden_dim*2, n_layers, dropout=dropout if n_layers > 1 else 0,\n                            batch_first=True)\n        self.attn = LuongAttention(hidden_dim)\n        self.fc = nn.Linear(hidden_dim*4, vocab_size)\n        self.hidden_dim = hidden_dim*2\n        self.n_layers = n_layers\n    def forward(self, tgt, hidden, encoder_outputs):\n        emb = self.embedding(tgt)\n        output, hidden = self.lstm(emb, hidden)\n        context, attn_w = self.attn(output.squeeze(1), encoder_outputs)\n        combined = torch.cat((output.squeeze(1), context), dim=1)\n        logits = self.fc(combined)\n        return logits, hidden, attn_w\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, hidden_dim):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.hidden_dim = hidden_dim\n    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n        encoder_outputs, (h, c) = self.encoder(src)\n        h_cat = torch.cat([h[-2], h[-1]], dim=1).unsqueeze(0).repeat(self.decoder.n_layers, 1, 1)\n        c_cat = torch.cat([c[-2], c[-1]], dim=1).unsqueeze(0).repeat(self.decoder.n_layers, 1, 1)\n        hidden = (h_cat, c_cat)\n        outputs = []\n        for t in range(tgt.size(1)-1):\n            input_t = tgt[:, t].unsqueeze(1)\n            if t > 0 and random.random() > teacher_forcing_ratio:\n                input_t = outputs[-1].argmax(-1)\n            logits, hidden, _ = self.decoder(input_t, hidden, encoder_outputs)\n            outputs.append(logits.unsqueeze(1))\n        return torch.cat(outputs, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:15:26.990324Z","iopub.execute_input":"2025-09-24T17:15:26.990651Z","iopub.status.idle":"2025-09-24T17:15:27.341728Z","shell.execute_reply.started":"2025-09-24T17:15:26.990621Z","shell.execute_reply":"2025-09-24T17:15:27.341120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import required modules\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nimport jiwer\n\n# Hyperparameter ranges\nPARAM_GRID = {\n    'emb_dim': [128, 256, 512],\n    'hidden_dim': [128, 256, 512],\n    'n_layers': [1, 2, 3],\n    'dropout': [0.1, 0.3, 0.5],\n    'lr': [1e-3, 5e-4, 1e-4],\n    'batch_size': [16, 32, 64],  # Reduced to avoid OOM\n    'teacher_forcing_ratio': [0.3, 0.5, 0.7]\n}\n\n# Random search\nN_TRIALS = 5\nbest_bleu = 0\nbest_params = None\nbest_model_path = '/kaggle/working/best_model.pt'\n\nfor trial in range(N_TRIALS):\n    print(f\"\\n=== Trial {trial+1}/{N_TRIALS} ===\")\n    params = {\n        'emb_dim': random.choice(PARAM_GRID['emb_dim']),\n        'hidden_dim': random.choice(PARAM_GRID['hidden_dim']),\n        'n_layers': random.choice(PARAM_GRID['n_layers']),\n        'dropout': random.choice(PARAM_GRID['dropout']),\n        'lr': random.choice(PARAM_GRID['lr']),\n        'batch_size': random.choice(PARAM_GRID['batch_size']),\n        'teacher_forcing_ratio': random.choice(PARAM_GRID['teacher_forcing_ratio'])\n    }\n    print(f\"Parameters: {params}\")\n\n    # DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, collate_fn=collate_fn, num_workers=0, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], collate_fn=collate_fn, num_workers=0, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], collate_fn=collate_fn, num_workers=0, pin_memory=True)\n    print(f\"Loaders ready: Train {len(train_loader)} batches, Val {len(val_loader)}, Test {len(test_loader)}\")\n\n    # Initialize model\n    encoder = Encoder(len(src_vocab.itos), params['emb_dim'], params['hidden_dim'], params['n_layers'], params['dropout'])\n    decoder = Decoder(len(tgt_vocab.itos), params['emb_dim'], params['hidden_dim'], params['n_layers'], params['dropout'])\n    model = Seq2Seq(encoder, decoder, params['hidden_dim']).to(device)\n\n    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    # Training function\n    def train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs=10, patience=3):\n        best_val_loss = float('inf')\n        patience_counter = 0\n        for epoch in range(n_epochs):\n            model.train()\n            train_loss = 0\n            batch_count = 0\n            for src, tgt in tqdm(train_loader, desc=f\"Trial {trial+1} Epoch {epoch+1}\"):\n                src = src.to(device)\n                tgt = tgt.to(device)\n                optimizer.zero_grad()\n                output = model(src, tgt, params['teacher_forcing_ratio'])\n                output = output.view(-1, output.size(-1))\n                tgt = tgt[:, 1:].reshape(-1)\n                loss = criterion(output, tgt)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n                train_loss += loss.item()\n                batch_count += 1\n            \n            train_loss /= batch_count\n            \n            model.eval()\n            val_loss = 0\n            batch_count = 0\n            with torch.no_grad():\n                for src, tgt in val_loader:\n                    src = src.to(device)\n                    tgt = tgt.to(device)\n                    output = model(src, tgt, 0)\n                    output = output.view(-1, output.size(-1))\n                    tgt = tgt[:, 1:].reshape(-1)\n                    loss = criterion(output, tgt)\n                    val_loss += loss.item()\n                    batch_count += 1\n            \n            val_loss /= batch_count\n            \n            print(f'Trial {trial+1} Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}')\n            \n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(model.state_dict(), f'/kaggle/working/model_trial_{trial+1}.pt')\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch+1}\")\n                    break\n        \n        model.load_state_dict(torch.load(f'/kaggle/working/model_trial_{trial+1}.pt'))\n        return model, best_val_loss\n\n    # Batch inference function\n    def translate_batch(model, src_tensor, max_len=20):\n        model.eval()\n        outputs = []\n        with torch.no_grad():\n            src_tensor = src_tensor.to(device)\n            encoder_outputs, (h, c) = model.encoder(src_tensor)\n            h_cat = torch.cat([h[-2], h[-1]], dim=1).unsqueeze(0).repeat(model.decoder.n_layers, 1, 1)\n            c_cat = torch.cat([c[-2], c[-1]], dim=1).unsqueeze(0).repeat(model.decoder.n_layers, 1, 1)\n            hidden = (h_cat, c_cat)\n            inputs = torch.tensor([1] * src_tensor.shape[0], device=device).unsqueeze(1)\n            \n            for _ in range(max_len):\n                logits, hidden, _ = model.decoder(inputs, hidden, encoder_outputs)\n                pred_tokens = logits.argmax(-1).unsqueeze(1)\n                outputs.append(pred_tokens)\n                if (pred_tokens == 2).all():\n                    break\n                inputs = pred_tokens\n            \n            outputs = torch.cat(outputs, dim=1)\n        return outputs.cpu().numpy().tolist()\n\n    # Evaluation function (fixed)\n    def evaluate_model(model, test_loader):\n        model.eval()\n        references = []\n        hypotheses = []\n        losses = []\n        \n        with torch.no_grad():\n            for i, (src, tgt) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n                src = src.to(device)\n                tgt = tgt.to(device)\n                output = model(src, tgt, 0)\n                output = output.view(-1, output.size(-1))\n                tgt_flat = tgt[:, 1:].reshape(-1)\n                loss = criterion(output, tgt_flat)\n                losses.append(loss.item())\n                \n                hypotheses_batch = translate_batch(model, src)\n                for j in range(src.shape[0]):\n                    src_j = src[j, 1:-1].cpu().numpy().tolist()\n                    tgt_j = tgt[j, 1:-1].cpu().numpy().tolist()\n                    hyp_j = hypotheses_batch[j]\n                    if len(tgt_j) == 0 or len(hyp_j) == 0:\n                        print(f\"Warning: Empty sequence at index {j}, skipping.\")\n                        continue\n                    # Fix: Use list indexing instead of .get()\n                    tgt_str = ' '.join([tgt_vocab.itos[idx] for idx in tgt_j if idx not in [0, 1, 2, 3] and idx < len(tgt_vocab.itos)])\n                    if any(ord(c) >= 0x0600 and ord(c) <= 0x06FF for c in tgt_str):\n                        print(f\"Warning: Urdu script detected in ground truth at index {j}, skipping.\")\n                        continue\n                    references.append([tgt_j])\n                    hypotheses.append(hyp_j)\n        \n        # Fix: Use list indexing for decoding\n        ref_strings = [' '.join([tgt_vocab.itos[idx] for idx in ref[0] if idx not in [0, 1, 2, 3] and idx < len(tgt_vocab.itos)]) for ref in references]\n        hyp_strings = [' '.join([tgt_vocab.itos[idx] for idx in hyp if idx not in [0, 1, 2, 3] and idx < len(tgt_vocab.itos)]) for hyp in hypotheses]\n        \n        perplexity = np.exp(np.mean(losses)) if losses else float('inf')\n        smooth = SmoothingFunction().method1\n        bleu = corpus_bleu(references, hypotheses, smoothing_function=smooth) if references else 0.0\n        cer_scores = [jiwer.cer(ref, hyp) for ref, hyp in zip(ref_strings, hyp_strings) if ref and hyp]\n        avg_cer = np.mean(cer_scores) if cer_scores else float('inf')\n        \n        print(f'Perplexity: {perplexity:.3f}, BLEU: {bleu:.3f}, CER: {avg_cer:.3f}')\n        \n        for i in range(min(5, len(test_urdu))):\n            # Fix: Use list indexing for decoding\n            urdu = ' '.join([src_vocab.itos[idx.item()] for idx in test_dataset[i][0][1:-1] if idx not in [0, 1, 2, 3] and idx < len(src_vocab.itos)])\n            ground_truth = ' '.join([tgt_vocab.itos[idx] for idx in test_dataset[i][1][1:-1].cpu().numpy().tolist() if idx not in [0, 1, 2, 3] and idx < len(tgt_vocab.itos)])\n            if any(ord(c) >= 0x0600 and ord(c) <= 0x06FF for c in ground_truth):\n                print(f\"Warning: Urdu script in ground truth at test pair {i}, skipping.\")\n                continue\n            hyp = translate_batch(model, test_dataset[i][0][1:-1].unsqueeze(0))[0]\n            predicted = ' '.join([tgt_vocab.itos[idx] for idx in hyp if idx not in [0, 1, 2, 3] and idx < len(tgt_vocab.itos)])\n            print(f\"\\nUrdu: {urdu}\\nGround Truth: {ground_truth}\\nPredicted: {predicted}\")\n        \n        return perplexity, bleu, avg_cer\n\n    # Train and evaluate\n    try:\n        model, val_loss = train_model(model, train_loader, val_loader, optimizer, criterion, n_epochs=10)\n        perplexity, bleu, cer = evaluate_model(model, test_loader)\n        \n        if bleu > best_bleu:\n            best_bleu = bleu\n            best_params = params\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"New best BLEU: {bleu:.3f}, saved to {best_model_path}\")\n    except RuntimeError as e:\n        print(f\"Trial {trial+1} failed with error: {e}\")\n        continue\n\nprint(f\"\\nBest Parameters: {best_params}\")\nprint(f\"Best BLEU: {best_bleu:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T17:15:30.313995Z","iopub.execute_input":"2025-09-24T17:15:30.314723Z","execution_failed":"2025-09-24T21:13:17.246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}